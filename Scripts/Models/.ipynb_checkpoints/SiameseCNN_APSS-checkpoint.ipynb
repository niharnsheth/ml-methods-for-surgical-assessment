{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Libraries   ---  #\n",
    "# File imports and aggregates data from multiple databases\n",
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv1D, MaxPool1D, Lambda\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e263201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the database files\n",
    "# source_path = os.getcwd() + '/../../Nihar/ML-data/SurgicalData'\n",
    "# source_path = os.getcwd() + \"/../../../../Nihar/ML-data/SurgicalData\"\n",
    "# source_path = os.getcwd() + '/../../../../Nihar/ML-data/SurgicalData/ManuallyCleaned_06262021'\n",
    "source_path = os.getcwd() + '/../../../../Nihar/ML-data/SurgicalData/Manually_Cleaned_And_Annotated_06272021'\n",
    "surgery_selected = 0\n",
    "#action_selected = 2\n",
    "\n",
    "surgery_name_list = ['/Pericardiocentesis',\n",
    "                     '/Thoracentesis']\n",
    "\n",
    "action_name_list = [['Chloraprep', 'Needle Insertion'],\n",
    "                    ['Chloraprep', 'Scalpel Incision', 'Trocar Insertion', 'Anesthetization']]\n",
    "\n",
    "input_folder = '/TrainingDataForComparison'\n",
    "save_to_folder = '/Results/Siamese'\n",
    "save_model = '/08212021_flexSCNN_w160_epc200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdeea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 ---  Define hyper parameters  ---  #\n",
    "skill_levels = 3\n",
    "\n",
    "no_sensors = 1\n",
    "features_per_sensor = 13\n",
    "n_features = no_sensors * features_per_sensor\n",
    "n_classes = 1  # number of outputs for classification\n",
    "epochs = [[200,200], [200,200,200,200]]\n",
    "# epochs = [[1,1],[1,1,1,1]]\n",
    "random_seed = 11\n",
    "# ##\n",
    "# # sliding_window = 200\n",
    "# sliding_window = [[140, 140], [140, 140, 140, 140]]\n",
    "# sliding_w_dim = 60\n",
    "# # window_step_size = 25\n",
    "# window_step_size = [[10,10], [10, 10, 10, 10]]\n",
    "# batch_size = 20\n",
    "# # batch_size = [[10, 10], [10, 10, 10, 10]]\n",
    "#\n",
    "# learning_rate = 0.001\n",
    "\n",
    "##\n",
    "sliding_window = [[160, 160], [160, 160, 160, 160]]\n",
    "sliding_w_dim = 60\n",
    "# window_step_size = 25\n",
    "window_step_size = [[10,10], [10, 10, 10, 10]]\n",
    "batch_size = 20\n",
    "# batch_size = [[10, 10], [10, 10, 10, 10]]\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  ---   Make motion windows   --- #\n",
    "def create_padding_df(num_of_rows, number_of_features):\n",
    "    labels_arr = np.zeros((num_of_rows, number_of_features))\n",
    "    return labels_arr\n",
    "\n",
    "\n",
    "# return an input and output data frame with motion windows\n",
    "def create_motion_windows(window_span, df_to_change, step_size, number_of_features, number_of_labels):\n",
    "    local_feature_df = []\n",
    "    local_label_df = []\n",
    "    header_list = [\"X\", \"Y\", \"Z\", \"W\", \"Qx\", \"Qy\", \"Qz\",\n",
    "                   \"Vx\", \"Vy\", \"Vz\", \"VQx\", \"VQy\", \"VQz\",\n",
    "                   \"Similarity\"]\n",
    "    # padding_df = pd.DataFrame(0, index=np.arange(window_span), columns=header_list)\n",
    "    padding_df = pd.DataFrame(np.zeros((window_span, number_of_features+number_of_labels)), columns=header_list)\n",
    "    # print(padding_df.shape)\n",
    "    # df_to_change = pd.concat([df_to_change, padding_df], axis=1, ignore_index=True)\n",
    "    df_to_change = df_to_change.append(padding_df, ignore_index=True)\n",
    "    steps = range(len(df_to_change) - window_span)\n",
    "    time_index = 0\n",
    "    while time_index + window_span < len(df_to_change):\n",
    "        a = df_to_change.iloc[time_index:time_index + window_span, :-number_of_labels].reset_index(drop=True).to_numpy()\n",
    "        # a.reset_index(drop=True)\n",
    "        # b = df_to_change.iloc[time_index + window_span, number_of_features:].reset_index(drop=True).to_numpy()\n",
    "        b = df_to_change.iloc[time_index, number_of_features:].reset_index(drop=True).to_numpy()\n",
    "        local_feature_df.append(a)\n",
    "        local_label_df.append(b)\n",
    "        time_index += step_size\n",
    "    return local_feature_df, local_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------   SIAMESE CONVOLUTION NEURAL NETWORK ----------------------------------------- #\n",
    "\n",
    "## --- Siamese network using CNNs --- #\n",
    "# left_input_performance = Input((sliding_window, n_features))\n",
    "# right_input_performance = Input((sliding_window, n_features))\n",
    "left_input_performance = Input((None, n_features))\n",
    "right_input_performance = Input((None, n_features))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=38, kernel_size=2, activation='relu', input_shape=(None, n_features)))\n",
    "model.add(MaxPool1D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters=76, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters=152, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters=308, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2, strides=2))\n",
    "model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "#model.add(Dense())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3080, activation='sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(760, activation='sigmoid'))\n",
    "\n",
    "encoded_left = model(left_input_performance)\n",
    "encoded_right = model(right_input_performance)\n",
    "# Getting the L1 Distance between the 2 encodings\n",
    "L1_layer = Lambda(lambda tensor: K.abs(tensor[0] - tensor[1]))\n",
    "# Add the distance function to the network\n",
    "L1_distance = L1_layer([encoded_left, encoded_right])\n",
    "prediction = Dense(1, activation='sigmoid')(L1_distance)\n",
    "siamese_net = Model(inputs=[left_input_performance, right_input_performance], outputs=prediction)\n",
    "optimizer = Adam(learning_rate=learning_rate, decay=2.5e-4)\n",
    "\n",
    "siamese_net.summary()\n",
    "siamese_net.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# opt = tf.keras.optimizers.Adam(lr=learning_rate, momentum=0.9, decay=1e-2/epochs)\n",
    "\n",
    "plot_model(siamese_net, show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- Training process  --- ##\n",
    "print(' ------------- Training   ------------')\n",
    "\n",
    "\n",
    "# Create folder to save the model and results\n",
    "os.mkdir(source_path + save_to_folder + save_model)\n",
    "# Create folder to save all the plots\n",
    "os.mkdir(source_path + save_to_folder + save_model + '/' + 'Graphs')\n",
    "\n",
    "# train for all surgical procedures\n",
    "# for surgery_selected in surgery_name_list[1]`:\n",
    "for surgery_selected in range(0, len(surgery_name_list)):\n",
    "\n",
    "    # train for each surgical task with procedure\n",
    "    surgical_tasks = os.listdir(source_path + input_folder + surgery_name_list[surgery_selected] + '/')\n",
    "    # for action_selected in surgical_tasks[0]:\n",
    "    for action_selected in range(len(surgical_tasks)):\n",
    "        print(\"Surgery: \" + surgery_name_list[surgery_selected] + \" -  Action: \" + surgical_tasks[action_selected])\n",
    "        csv_list = [f for f in os.listdir(source_path + input_folder +\n",
    "                                          surgery_name_list[surgery_selected] + '/' +\n",
    "                                          surgical_tasks[action_selected] + '/')\n",
    "                    if fnmatch.fnmatch(f, '*.csv')]\n",
    "\n",
    "        # initialize input and output list\n",
    "        feature_list_main = list()\n",
    "        label_list_main = list()\n",
    "\n",
    "        # train for surgical task\n",
    "        for file in csv_list:\n",
    "            # read the file into a data-frame\n",
    "            df = pd.read_csv(source_path + input_folder +\n",
    "                             surgery_name_list[surgery_selected] +\n",
    "                             '/' + surgical_tasks[action_selected] +\n",
    "                             '/' + file)\n",
    "            # remove null value rows from sample\n",
    "            check_if_null = df.isnull().values.any()\n",
    "            # print(file + \" has null values: \" + str(check_if_null))\n",
    "            df = df.dropna(how='any', axis=0)\n",
    "\n",
    "            # create motion windows and separate data into input and output\n",
    "            feature_list, label_list = create_motion_windows(sliding_window[surgery_selected][action_selected],\n",
    "                                                             df,\n",
    "                                                             window_step_size[surgery_selected][action_selected],\n",
    "                                                             features_per_sensor,\n",
    "                                                             n_classes)\n",
    "            # feature_list, label_list = create_motion_windows(sliding_window, df,\n",
    "            #                                                  window_step_size,\n",
    "            #                                                  features_per_sensor,\n",
    "            #                                                  n_classes)\n",
    "            # create list of windows\n",
    "            feature_list_main.extend(feature_list)\n",
    "            label_list_main.extend(label_list)\n",
    "\n",
    "        # create input and output list\n",
    "        sorting_list = list(zip(feature_list_main, label_list_main))\n",
    "\n",
    "        # sort and count the windows based on labels\n",
    "        count_experts = 0\n",
    "        count_amateurs = 0\n",
    "        expert_list = []\n",
    "        amateur_list = []\n",
    "        for _ in range(0, len(sorting_list)):\n",
    "            if sorting_list[_][1] == 1:\n",
    "                expert_list.append(list(sorting_list[_]))\n",
    "                count_experts += 1\n",
    "            else:\n",
    "                amateur_list.append(list(sorting_list[_]))\n",
    "                count_amateurs += 1\n",
    "        print(\"Total expert samples: \" + str(count_experts) + \" -  Total amateur samples: \" + str(count_amateurs))\n",
    "        # remove the excess windows to keep same number of input data per label\n",
    "        excess_samples = count_experts - count_amateurs\n",
    "        if excess_samples > 0:\n",
    "            # excess_samples = count_experts - count_amateurs\n",
    "            for i in range(excess_samples):\n",
    "                expert_list.pop(random.randint(0, len(expert_list) - 1))\n",
    "        else:\n",
    "            # excess_samples = count_amateurs - count_experts\n",
    "            for i in range(abs(excess_samples)):\n",
    "                amateur_list.pop(random.randint(0, len(amateur_list) - 1))\n",
    "\n",
    "        # drop all the previous labels\n",
    "        for i in range(len(expert_list)):\n",
    "            expert_list[i] = np.asarray(expert_list[i][0])\n",
    "            amateur_list[i] = np.asarray(amateur_list[i][0])\n",
    "\n",
    "        # shuffle samples\n",
    "        random.seed(random_seed)\n",
    "        random.shuffle(expert_list)\n",
    "        random.shuffle(amateur_list)\n",
    "\n",
    "        # creating pairs to train model\n",
    "        left_input = []\n",
    "        right_input = []\n",
    "        targets = []\n",
    "        # number of pairs per image\n",
    "        pairs = 1\n",
    "        # get length of input data\n",
    "        len_individual_input = len(expert_list)\n",
    "        # create pairs with labels\n",
    "        for i in range(0, len(expert_list)):\n",
    "            for _ in range(pairs):\n",
    "                # assign the current window index\n",
    "                compare_to = i\n",
    "                # making sure not to pair with itself\n",
    "                while compare_to == i:\n",
    "                    compare_to = random.randint(0, len(expert_list)-1)\n",
    "                # create positive pair\n",
    "                left_input.append(expert_list[i])\n",
    "                right_input.append(expert_list[compare_to])\n",
    "                targets.append(1.0)\n",
    "                # create negative pair\n",
    "                left_input.append(expert_list[i])\n",
    "                right_input.append(amateur_list[compare_to])\n",
    "                targets.append(0.0)\n",
    "\n",
    "        # combine to shuffle\n",
    "        data_pairs = list(zip(left_input, right_input, targets))\n",
    "        random.shuffle(data_pairs)\n",
    "        print(\"Total number of pairs are: \" + str(len(data_pairs)))\n",
    "        # split data for testing\n",
    "        test_list = []\n",
    "        for i in range(0, int(len(data_pairs)/20)):\n",
    "            idx = random.randint(0, len(data_pairs)-1)\n",
    "            test_list.append(data_pairs[idx])\n",
    "            data_pairs.pop(idx)\n",
    "\n",
    "        # split input pairs and labels\n",
    "        left_input, right_input, targets = list(zip(*data_pairs))\n",
    "        test_left, test_right, test_targets = list(zip(*test_list))\n",
    "\n",
    "        # reshape to input for model\n",
    "        left_input = np.reshape(left_input, (len(left_input),\n",
    "                                             sliding_window[surgery_selected][action_selected],\n",
    "                                             n_features))\n",
    "        right_input = np.reshape(right_input, (len(right_input),\n",
    "                                               sliding_window[surgery_selected][action_selected],\n",
    "                                               n_features))\n",
    "        # left_input = np.reshape(left_input, (len(left_input),\n",
    "        #                                      sliding_window,\n",
    "        #                                      n_features))\n",
    "        # right_input = np.reshape(right_input, (len(right_input),\n",
    "        #                                        sliding_window,\n",
    "        #                                        n_features))\n",
    "        targets = np.array(targets)\n",
    "\n",
    "        test_left = np.reshape(test_left, (len(test_left),\n",
    "                                           sliding_window[surgery_selected][action_selected],\n",
    "                                           n_features))\n",
    "        test_right = np.reshape(test_right, (len(test_right),\n",
    "                                             sliding_window[surgery_selected][action_selected],\n",
    "                                             n_features))\n",
    "        # test_left = np.reshape(test_left, (len(test_left),\n",
    "        #                                    sliding_window,\n",
    "        #                                    n_features))\n",
    "        # test_right = np.reshape(test_right, (len(test_right),\n",
    "        #                                      sliding_window,\n",
    "        #                                      n_features))\n",
    "        test_targets = np.array(test_targets)\n",
    "\n",
    "        siamese_net.summary()\n",
    "        history = siamese_net.fit([left_input, right_input], targets,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=epochs[surgery_selected][action_selected],\n",
    "                                  verbose=1,\n",
    "                                  validation_data=([test_left, test_right], test_targets))\n",
    "\n",
    "        # Create folder to save trained model\n",
    "        os.mkdir(source_path + save_to_folder + save_model + '/' +\n",
    "                 surgery_name_list[surgery_selected] + '_' + surgical_tasks[action_selected])\n",
    "        # save the trained model\n",
    "        siamese_net.save(source_path + save_to_folder + save_model +\n",
    "                         '/' + surgery_name_list[surgery_selected] +\n",
    "                         '_' + surgical_tasks[action_selected] + '/', save_format='tf')\n",
    "\n",
    "        # save the history of training to csv file\n",
    "        train_hist_df = pd.DataFrame(history.history)\n",
    "        train_hist_df.to_csv(source_path + save_to_folder + save_model + '/'\n",
    "                             + surgery_name_list[surgery_selected] + '_'\n",
    "                             + str(action_selected) + '/'\n",
    "                             + action_name_list[surgery_selected][action_selected] + '.csv', index=False)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.title('Model loss - ' + surgery_name_list[surgery_selected] +\n",
    "                  ' - ' + action_name_list[surgery_selected][action_selected])\n",
    "        plt.ylabel('loss value')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.savefig(source_path + save_to_folder + save_model + '/' + 'Graphs' + '/' +\n",
    "                    surgery_name_list[surgery_selected] + '_' + surgical_tasks[action_selected] + '_' + 'loss' + '.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['accuracy'], label='acc')\n",
    "        plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "        plt.title('Model accuracy - ' + surgery_name_list[surgery_selected] +\n",
    "                  ' - ' + action_name_list[surgery_selected][action_selected])\n",
    "        plt.ylabel('accuracy value')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.savefig(source_path + save_to_folder + save_model + '/' + 'Graphs' + '/' +\n",
    "                    surgery_name_list[surgery_selected] + '_' + surgical_tasks[action_selected] + '_' + 'acc' + '.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity_python_zeromq",
   "language": "python",
   "name": "unity_python_zeromq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
